{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-21T11:08:38.532978Z",
     "start_time": "2024-07-21T11:08:38.420501Z"
    }
   },
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from RL.Parameters import *\n",
    "from RL.Environment import *"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T11:08:38.571681Z",
     "start_time": "2024-07-21T11:08:38.548999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MonteCarloESAgent:\n",
    "    def __init__(self, environment, gamma=0.9, epsilon=0.1, alpha=0.1):\n",
    "        self.environment = environment\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.policy = np.zeros(np.prod(environment.size), dtype=int)\n",
    "        self.state_values = np.zeros(np.prod(environment.size))\n",
    "        self.returns = {state: [] for state in range(np.prod(environment.size))}\n",
    "        self.initialize_policy()\n",
    "        \n",
    "    def initialize_policy(self):\n",
    "        for state in range(np.prod(self.environment.size)):\n",
    "            self.policy[state] = np.random.choice(range(len(self.environment.actions)))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.environment.actions)\n",
    "        else:\n",
    "            return self.environment.actions[self.policy[state]]\n",
    "\n",
    "    def generate_episode(self, max_steps=100):\n",
    "        state = self.environment.reset()\n",
    "        episode = []\n",
    "        steps = 0\n",
    "        while steps < max_steps:\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = self.environment.step(state, action)\n",
    "            episode.append((state, action, reward, next_state, done))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        return episode\n",
    "\n",
    "    def monte_carlo_es(self, episodes=20):\n",
    "        for i in range(episodes):\n",
    "            print(f\"Generating episode {i+1}...\")\n",
    "            episode = self.generate_episode()\n",
    "            G = 0\n",
    "            for t in range(len(episode) - 1, -1, -1):\n",
    "                state, action, reward, next_state, done = episode[t]\n",
    "                G = self.gamma * G + reward\n",
    "                self.returns[state].append(G)\n",
    "                self.state_values[state] = self.state_values[state] + self.alpha * (G - self.state_values[state])\n",
    "                if np.random.rand() < self.epsilon:\n",
    "                    self.policy[state] = np.random.choice(range(len(self.environment.actions)))\n",
    "                else:\n",
    "                    self.policy[state] = np.argmax([self.state_values[next_state] for next_state in self.environment.get_next_states(state)])\n",
    "\n",
    "    def find_best_path_for_goal(self, start_state):\n",
    "        path = []\n",
    "        current_state = start_state\n",
    "        while current_state != self.environment.goal_state:\n",
    "            path.append(current_state)\n",
    "            current_action = self.policy[current_state]\n",
    "            current_state = self.environment.get_next_state(current_state, current_action)\n",
    "        path.append(self.environment.goal_state)\n",
    "        return path"
   ],
   "id": "e10f6ea5db34920d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T11:08:38.855303Z",
     "start_time": "2024-07-21T11:08:38.586072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Liste des environnements : LineWorld, GridWorld, TwoRoundRockPaperScissors, MontyHallLevel1, MontyHallLevel2\n",
    "env_name = 'LineWorld' \n",
    "print(\"Algorithme choisi : Monte Carlo Off Policy Agent\")\n",
    "print(\"Environnement choisi :\", env_name)\n",
    "\n",
    "param = Parameters(env_name)\n",
    "environment = Environment(param.size, param.goal_state, param.rewards)\n",
    "\n",
    "monte_carlo_agent = MonteCarloESAgent(environment, gamma=0.9)\n",
    "monte_carlo_agent.monte_carlo_es(episodes=1)\n",
    "\n",
    "print(\"Valeurs des états (Monte Carlo ES Agent):\")\n",
    "print(monte_carlo_agent.state_values.reshape(param.size))\n",
    "print(\"\\nPolitique (Monte Carlo Agent):\")\n",
    "if len(param.size) == 1:\n",
    "    for state in range(param.size[0]):\n",
    "        if state == param.goal_state:\n",
    "            print(\" G \", end=\" \")\n",
    "        else:\n",
    "            print(monte_carlo_agent.policy[state], end=\" \")\n",
    "        print()\n",
    "else:\n",
    "    for row in range(param.size[0]):\n",
    "        for col in range(param.size[1]):\n",
    "            state = row * param.size[1] + col\n",
    "            if state == param.goal_state:\n",
    "                print(\" G \", end=\" \")\n",
    "            else:\n",
    "                print(monte_carlo_agent.policy[state], end=\" \")\n",
    "        print()\n",
    "\n",
    "start_state = 0\n",
    "best_path = monte_carlo_agent.find_best_path_for_goal(start_state)\n",
    "print(\"\\nMeilleur chemin (Monte Carlo ES Agent) de l'état 0 à l'objectif:\")\n",
    "print(best_path)"
   ],
   "id": "b433ca1afbf81851",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating episode 1...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m environment \u001B[38;5;241m=\u001B[39m Environment(param\u001B[38;5;241m.\u001B[39msize, param\u001B[38;5;241m.\u001B[39mgoal_state, param\u001B[38;5;241m.\u001B[39mrewards)\n\u001B[1;32m      7\u001B[0m monte_carlo_agent \u001B[38;5;241m=\u001B[39m MonteCarloESAgent(environment, gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m)\n\u001B[0;32m----> 8\u001B[0m monte_carlo_agent\u001B[38;5;241m.\u001B[39mmonte_carlo_es(episodes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValeurs des états (Monte Carlo ES Agent):\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(monte_carlo_agent\u001B[38;5;241m.\u001B[39mstate_values\u001B[38;5;241m.\u001B[39mreshape(param\u001B[38;5;241m.\u001B[39msize))\n",
      "Cell \u001B[0;32mIn[2], line 39\u001B[0m, in \u001B[0;36mMonteCarloESAgent.monte_carlo_es\u001B[0;34m(self, episodes)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(episodes):\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenerating episode \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 39\u001B[0m     episode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_episode()\n\u001B[1;32m     40\u001B[0m     G \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(episode) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n",
      "Cell \u001B[0;32mIn[2], line 28\u001B[0m, in \u001B[0;36mMonteCarloESAgent.generate_episode\u001B[0;34m(self, max_steps)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m steps \u001B[38;5;241m<\u001B[39m max_steps:\n\u001B[1;32m     27\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mselect_action(state)\n\u001B[0;32m---> 28\u001B[0m     next_state, reward, done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvironment\u001B[38;5;241m.\u001B[39mstep(state, action)\n\u001B[1;32m     29\u001B[0m     episode\u001B[38;5;241m.\u001B[39mappend((state, action, reward, next_state, done))\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m done:\n",
      "File \u001B[0;32m~/work/esgi/4IADB2/deep_reinforcement_learning/deep_reinforcement_learning_project/RL_project/RL/Environment.py:18\u001B[0m, in \u001B[0;36mEnvironment.step\u001B[0;34m(self, state, action)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, state, action):\n\u001B[0;32m---> 18\u001B[0m     next_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_next_state(state, action)\n\u001B[1;32m     19\u001B[0m     reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_reward(next_state)\n\u001B[1;32m     20\u001B[0m     done \u001B[38;5;241m=\u001B[39m next_state \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgoal_state\n",
      "File \u001B[0;32m~/work/esgi/4IADB2/deep_reinforcement_learning/deep_reinforcement_learning_project/RL_project/RL/Environment.py:24\u001B[0m, in \u001B[0;36mEnvironment.get_next_state\u001B[0;34m(self, state, action)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_next_state\u001B[39m(\u001B[38;5;28mself\u001B[39m, state, action):\n\u001B[0;32m---> 24\u001B[0m     row, col \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdivmod\u001B[39m(state, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msize[\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m action \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mup\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     26\u001B[0m         row \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(row \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[0;31mIndexError\u001B[0m: tuple index out of range"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ff168c702b61f118",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
