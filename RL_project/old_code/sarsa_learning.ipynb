{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:47:44.192629Z",
     "start_time": "2024-07-24T09:47:44.180295Z"
    }
   },
   "cell_type": "code",
   "source": "import random",
   "id": "a4b709b7b08c784b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:47:44.236635Z",
     "start_time": "2024-07-24T09:47:44.222744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SarsaLearning:\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "        self.start_state = 0\n",
    "        self.end_state = length - 1\n",
    "        self.actions = ['LEFT', 'RIGHT']\n",
    "\n",
    "        self.alpha = 0.1  # Learning rate\n",
    "        self.gamma = 0.9  # Discount factor\n",
    "        self.epsilon = 0.1  # Exploration rate\n",
    "        # self.q_table = {state: {action: 0.0 for action in self.actions} for state in range(self.length)}\n",
    "        self.q_table = {-2: {'LEFT': 0.0, 'RIGHT': 0.0}, -1: {'LEFT': 0.0, 'RIGHT': 0.0},\n",
    "                        0: {'LEFT': 0.0, 'RIGHT': 0.0}, 1: {\n",
    "                'LEFT': 0.0, 'RIGHT': 0.0}, 2: {'LEFT': 0.0, 'RIGHT': 0.0}}\n",
    "\n",
    "        print(self.q_table)\n",
    "\n",
    "    def get_next_state(self, state, action):\n",
    "        if action == 'LEFT':\n",
    "            if state == -2:\n",
    "                return -1\n",
    "\n",
    "            return state - 1\n",
    "        elif action == 'RIGHT':\n",
    "            if state == 2:\n",
    "                return 1\n",
    "\n",
    "            return state + 1\n",
    "\n",
    "    def get_reward(self, state):\n",
    "        if state == -2:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        return state == -2\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(self.actions)\n",
    "        else:\n",
    "            return max(self.q_table[state], key=self.q_table[state].get)\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state, next_action):\n",
    "        old_q_value = self.q_table[state][action]\n",
    "        next_q_value = self.q_table[next_state][next_action]\n",
    "        new_q_value = old_q_value + self.alpha * (reward + self.gamma * next_q_value - old_q_value)\n",
    "        self.q_table[state][action] = new_q_value\n",
    "\n",
    "    def train(self, episodes):\n",
    "        for episode in range(episodes):\n",
    "            state = self.start_state\n",
    "            action = self.choose_action(state)\n",
    "            while not self.is_terminal(state):\n",
    "                next_state = self.get_next_state(state, action)\n",
    "                reward = self.get_reward(next_state)\n",
    "                next_action = self.choose_action(next_state)\n",
    "                self.update_q_value(state, action, reward, next_state, next_action)\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        return max(self.q_table[state], key=self.q_table[state].get)\n",
    "\n",
    "    def print_q_table(self):\n",
    "        for state in range(-2, 2):\n",
    "            print(f\"State {state}: {self.q_table[state]}\")\n",
    "\n",
    "    def print_optimal_policy(self):\n",
    "        for state in range(-2, 2):\n",
    "            if self.is_terminal(state):\n",
    "                print(f\"State {state}: Goal\")\n",
    "            else:\n",
    "                best_action = self.get_best_action(state)\n",
    "                print(f\"State {state}: Best action = {best_action}\")"
   ],
   "id": "a0b6e1595a396cc2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:47:44.244962Z",
     "start_time": "2024-07-24T09:47:44.238974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "q = SarsaLearning(1000)\n",
    "q.train(100)\n",
    "q.print_q_table()\n",
    "q.print_optimal_policy()"
   ],
   "id": "3ee64ee4a6ffb055",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-2: {'LEFT': 0.0, 'RIGHT': 0.0}, -1: {'LEFT': 0.0, 'RIGHT': 0.0}, 0: {'LEFT': 0.0, 'RIGHT': 0.0}, 1: {'LEFT': 0.0, 'RIGHT': 0.0}, 2: {'LEFT': 0.0, 'RIGHT': 0.0}}\n",
      "State -2: {'LEFT': 0.0, 'RIGHT': 0.0}\n",
      "State -1: {'LEFT': 0.9999734386011123, 'RIGHT': 0.22712169157471837}\n",
      "State 0: {'LEFT': 0.8965085071648758, 'RIGHT': 0.04013218030692066}\n",
      "State 1: {'LEFT': 0.2743767040417963, 'RIGHT': 0.0}\n",
      "State -2: Goal\n",
      "State -1: Best action = LEFT\n",
      "State 0: Best action = LEFT\n",
      "State 1: Best action = LEFT\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
