{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-21T17:08:03.539970Z",
     "start_time": "2024-07-21T17:08:03.536501Z"
    }
   },
   "source": [
    "import random\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T17:08:03.545352Z",
     "start_time": "2024-07-21T17:08:03.540964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Environment:\n",
    "    def __init__(self, size, goal_state, rewards):\n",
    "        self.size = size\n",
    "        self.goal_state = goal_state\n",
    "        self.rewards = rewards\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.action_space = len(self.actions)\n",
    "        self.current_state = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = 0\n",
    "        return self.current_state\n",
    "\n",
    "    def get_next_state(self, state, action):\n",
    "        row, col = divmod(state, self.size[1])\n",
    "        if action == 'up':\n",
    "            row = max(row - 1, 0)\n",
    "        elif action == 'down':\n",
    "            row = min(row + 1, self.size[0] - 1)\n",
    "        elif action == 'left':\n",
    "            col = max(col - 1, 0)\n",
    "        elif action == 'right':\n",
    "            col = min(col + 1, self.size[1] - 1)\n",
    "        return row * self.size[1] + col\n",
    "\n",
    "    def get_reward(self, state):\n",
    "        return self.rewards.get(state, -1)\n",
    "    \n",
    "    def step(self, action):\n",
    "        next_state = self.get_next_state(self.current_state, action)\n",
    "        reward = self.get_reward(next_state)\n",
    "        done = self.is_done(next_state)\n",
    "        self.current_state = next_state\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def is_done(self, state):\n",
    "        return state == self.goal_state"
   ],
   "id": "63ad574b03acf0a1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T17:08:03.548812Z",
     "start_time": "2024-07-21T17:08:03.546104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Parameters:\n",
    "    def __init__(self, size, goal_state, rewards, gamma):\n",
    "        self.size = size\n",
    "        self.goal_state = goal_state\n",
    "        self.rewards = rewards\n",
    "        self.gamma = gamma"
   ],
   "id": "40e54698e349ae0c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T17:08:03.555775Z",
     "start_time": "2024-07-21T17:08:03.549868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DynaQAgent:\n",
    "    def __init__(self, environment, gamma=0.9, alpha=0.1, epsilon=0.1, n=10):\n",
    "        self.environment = environment\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.n = n\n",
    "        self.q_values = np.zeros((np.prod(environment.size), environment.action_space))\n",
    "        self.model = {}\n",
    "        self.initialize_model()\n",
    "\n",
    "    def initialize_model(self):\n",
    "        for state in range(np.prod(self.environment.size)):\n",
    "            for action in range(self.environment.action_space):\n",
    "                self.model[(state, action)] = (0, state)  # Initialize with zero reward and same state\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.environment.action_space)\n",
    "        else:\n",
    "            return np.argmax(self.q_values[state])\n",
    "\n",
    "    def update_model(self, state, action, reward, next_state):\n",
    "        self.model[(state, action)] = (reward, next_state)\n",
    "\n",
    "    def planning_step(self):\n",
    "        for _ in range(self.n):\n",
    "            state, action = random.choice(list(self.model.keys()))\n",
    "            reward, next_state = self.model[(state, action)]\n",
    "            self.q_values[state][action] += self.alpha * (reward + self.gamma * np.max(self.q_values[next_state]) - self.q_values[state][action])\n",
    "\n",
    "    def dyna_q(self, episodes=100):\n",
    "        for _ in range(episodes):\n",
    "            state = self.environment.reset()\n",
    "            while True:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _ = self.environment.step(self.environment.actions[action])\n",
    "                self.q_values[state][action] += self.alpha * (reward + self.gamma * np.max(self.q_values[next_state]) - self.q_values[state][action])\n",
    "                self.update_model(state, action, reward, next_state)\n",
    "                self.planning_step()\n",
    "                if done:\n",
    "                    break\n",
    "                state = next_state\n",
    "\n",
    "    def find_best_path_for_goal(self, start_state):\n",
    "        state = start_state\n",
    "        path = [state]\n",
    "        action_map = {0: 'up', 1: 'down', 2: 'left', 3: 'right'}\n",
    "        while True:\n",
    "            action_idx = np.argmax(self.q_values[state])\n",
    "            action = action_map[action_idx]\n",
    "            next_state, reward, done, _ = self.environment.step(action)\n",
    "            path.append(next_state)\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        return path"
   ],
   "id": "990ab5749feea96a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T17:08:03.576065Z",
     "start_time": "2024-07-21T17:08:03.556424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param = Parameters((3, 3), 8, {8: 10, 3: -5}, 0.9)\n",
    "environment = Environment(param.size, param.goal_state, param.rewards)\n",
    "agent = DynaQAgent(environment, param.gamma)\n",
    "\n",
    "agent.dyna_q()\n",
    "\n",
    "print(\"State Values:\")\n",
    "print(agent.q_values.max(axis=1).reshape(param.size))\n",
    "print(\"\\nPolicy:\")\n",
    "for row in range(param.size[0]):\n",
    "    for col in range(param.size[1]):\n",
    "        state = row * param.size[1] + col\n",
    "        if state == param.goal_state:\n",
    "            print(\" G \", end=\" \")\n",
    "        else:\n",
    "            action_idx = np.argmax(agent.q_values[state])\n",
    "            print(environment.actions[action_idx], end=\" \")\n",
    "    print()\n",
    "\n",
    "start_state = 0\n",
    "best_path = agent.find_best_path_for_goal(start_state)\n",
    "print(\"\\nBest Path from state 0 to goal:\")\n",
    "print(best_path)"
   ],
   "id": "b433ca1afbf81851",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Values:\n",
      "[[ 4.57999956  6.19999984  7.99999999]\n",
      " [ 6.1999443   7.99999373 10.        ]\n",
      " [ 0.57862161  6.19988552  0.        ]]\n",
      "\n",
      "Policy:\n",
      "right right down \n",
      "right right down \n",
      "up up  G  \n",
      "\n",
      "Best Path from state 0 to goal:\n",
      "[0, 8]\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
